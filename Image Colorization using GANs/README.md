# ğŸ¨ Image Colorization using Conditional GANs (Pix2Pix)

This project implements a **deep learning-based image colorization model** using a **Conditional Generative Adversarial Network (cGAN)**, inspired by the Pix2Pix architecture. It takes grayscale images as input and generates their realistic color versions.

---

## ğŸ“Œ Features

- âœ… Pix2Pix-style conditional GAN
- âœ… Custom Generator and Discriminator networks
- âœ… Combined L1 Loss and GAN Loss for stable training
- âœ… PatchGAN discriminator for high-frequency details
- âœ… Input image resolution: **128Ã—128**
- âœ… Full training and inference pipelines
- âœ… Gradient clipping and learning rate scheduling
- âœ… Model checkpoint saving and sample image generation per epoch

---

## ğŸ§  Model Architecture

### ğŸ§¬ Generator (U-Net Inspired)

- Encoder-decoder architecture
- Multiple convolutional â†’ batch norm â†’ ReLU layers
- Skip connections (optional for improved performance)
- Translates 1-channel grayscale input to 3-channel RGB color image

### ğŸ” Discriminator (PatchGAN)

- Focuses on local image patches (70Ã—70) instead of the entire image
- Concatenates the grayscale input and the corresponding color output
- Outputs a matrix of real/fake predictions for each patch

---

## ğŸ—‚ï¸ Dataset

- Input: Grayscale images (1 channel)
- Output: Corresponding color images (3 channels)
- All images resized to **128Ã—128**

---

## ğŸš€ Training Strategy

- Optimizers: Adam (lr=0.0002, betas=(0.5, 0.999))
- Losses:
  - **Adversarial Loss** (BCE)
  - **Reconstruction Loss** (L1)
- Learning Rate Scheduler: StepLR
- Gradient clipping for stable training
